{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Μέλη Ομάδας:\n",
    "- Ευάγγελος Λευτάκης : 3200093\n",
    "- Ρέα Σκλήκα : 3210192 \n",
    "- Σοφία Σωτηρίου : 3210181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We Prepare the train, dev and test data and create a binary representation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3834\n",
      "(25000, 3834)\n",
      "  (0, 419)\t1\n",
      "  (0, 3420)\t1\n",
      "  (0, 1308)\t1\n",
      "  (0, 3689)\t1\n",
      "  (0, 1864)\t1\n",
      "  (0, 451)\t1\n",
      "  (0, 542)\t1\n",
      "  (0, 2011)\t1\n",
      "  (0, 2902)\t1\n",
      "  (0, 3217)\t1\n",
      "  (0, 954)\t1\n",
      "  (0, 1160)\t1\n",
      "  (0, 2714)\t1\n",
      "  (0, 3285)\t1\n",
      "  (0, 3394)\t1\n",
      "  (0, 2438)\t1\n",
      "  (0, 3409)\t1\n",
      "  (0, 2517)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 3824)\t1\n",
      "  (0, 761)\t1\n",
      "  (0, 1712)\t1\n",
      "  (0, 350)\t1\n",
      "  (0, 3406)\t1\n",
      "  (0, 2834)\t1\n",
      "  :\t:\n",
      "  (0, 2400)\t1\n",
      "  (0, 1996)\t1\n",
      "  (0, 3415)\t1\n",
      "  (0, 336)\t1\n",
      "  (0, 3173)\t1\n",
      "  (0, 2516)\t1\n",
      "  (0, 3400)\t1\n",
      "  (0, 141)\t1\n",
      "  (0, 1520)\t1\n",
      "  (0, 3600)\t1\n",
      "  (0, 3271)\t1\n",
      "  (0, 370)\t1\n",
      "  (0, 3743)\t1\n",
      "  (0, 488)\t1\n",
      "  (0, 3408)\t1\n",
      "  (0, 3017)\t1\n",
      "  (0, 323)\t1\n",
      "  (0, 997)\t1\n",
      "  (0, 995)\t1\n",
      "  (0, 2042)\t1\n",
      "  (0, 3528)\t1\n",
      "  (0, 3103)\t1\n",
      "  (0, 1980)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 3607)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "(x_train_imdb, y_train), (x_test_imdb, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "split_index = int(0.8 * len(x_train_imdb))  # 80% for training, 20% for dev\n",
    "\n",
    "# x_train_imdb, y_train_imdb = temp_x_train_imdb[:split_index], y_train[:split_index]\n",
    "# x_dev_imdb, y_dev_imdb = temp_x_train_imdb[split_index:], y_train[split_index:]\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train_imdb = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train_imdb])\n",
    "x_test_imdb = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test_imdb])\n",
    "\n",
    "binary_vectorizer = CountVectorizer(binary=True, min_df=100)\n",
    "x_train = binary_vectorizer.fit_transform(x_train_imdb)\n",
    "x_test = binary_vectorizer.transform(x_test_imdb)\n",
    "print(\n",
    "    'Vocabulary size:', len(binary_vectorizer.vocabulary_)\n",
    ")\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(x_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "   \n",
    "\n",
    "    def __init__(self,epochs=20,learning_rate=0.01):\n",
    "        self.epochs=epochs;\n",
    "        self.learning_rate=learning_rate;\n",
    "        self.weights = np.array([])\n",
    "        \n",
    "    \n",
    "\n",
    "    def sigmoid(t):\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "    def fit(self,x_train_input,y_train_input):\n",
    "        ''' '''\n",
    "        feature_vector_length = x_train_input.shape[1]\n",
    "        split_index = int(0.8 * x_train_input.shape[0])  # 80% for training, 20% for dev\n",
    "\n",
    "        x_train, y_train = x_train_input[:split_index], y_train_input[:split_index]\n",
    "        x_dev, y_dev= x_train_input[split_index:], y_train_input[split_index:]\n",
    "        \n",
    "        # x_train = np.insert(x_train, 0, 1, axis=1)\n",
    "        print(x_train.shape)\n",
    "        print(x_dev.shape)\n",
    "        self.weights = self.initializeWeights(feature_vector_length)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(x_train.shape(0)):\n",
    "                print(\"\");\n",
    "                # for weightIndex in range(self.weights):\n",
    "                    \n",
    "                #     currentWeight = self.weights[weightIndex]\n",
    "                    \n",
    "                #     weightChange = self.learning_rate * \n",
    "                # self.weights[weightIndex] = currentWeight + weightChange\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        recall, precision = 0\n",
    "        return (recall, precision)\n",
    "\n",
    "    def SGC(self,w):\n",
    "        # return w + self.learning_rate*1\n",
    "        print(\"parameter estimation\")\n",
    "\n",
    "    @staticmethod\n",
    "    def initializeWeights(size):\n",
    "        return np.random.randn(size) * 0.01\n",
    "    \n",
    "    def logLikelyhood():\n",
    "        sum = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Here we try out the Logistic Regression learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m LogReg \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mLogReg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LogReg.Predict(x_test)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 23\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, x_train_input, y_train_input)\u001b[0m\n\u001b[0;32m     20\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m x_train_input[:split_index], y_train_input[:split_index]\n\u001b[0;32m     21\u001b[0m x_dev, y_dev\u001b[38;5;241m=\u001b[39m x_train_input[split_index:], y_train_input[split_index:]\n\u001b[1;32m---> 23\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_dev\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minsert\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vleft\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:5356\u001b[0m, in \u001b[0;36minsert\u001b[1;34m(arr, obj, values, axis)\u001b[0m\n\u001b[0;32m   5354\u001b[0m     axis \u001b[38;5;241m=\u001b[39m ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   5355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5356\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5357\u001b[0m slobj \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)]\u001b[38;5;241m*\u001b[39mndim\n\u001b[0;32m   5358\u001b[0m N \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[axis]\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression(100,0.01)\n",
    "LogReg.fit(x_train,y_train)\n",
    "\n",
    "# LogReg.Predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
